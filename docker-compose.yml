#
# This docker-compose file starts and runs:
#  kafka cluster
#  zookeeper instance
#  producer_server
#  spark streaming

version: '3.7'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.2.2
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: "2181"

  kafka0:
    image: confluentinc/cp-kafka:5.2.2
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka0:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
    depends_on:
      - "zookeeper"

  producer:
    build:
      dockerfile: docker/producer/Dockerfile
      context: .
    depends_on:
      - "kafka0"
    # use host mode to avoid dealing with networking
    network_mode: "host"
  #    volumes:
  #      - ./src:/src/

  spark:
    build:
      dockerfile: docker/spark/Dockerfile
      context: .
    depends_on:
      - "producer"
    # use host mode to avoid dealing with networking
    network_mode: "host"
    ports:
      - 4040:4040
    command: [ "spark-submit", "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1", "--master", "local[*]","data_stream.py" ]
    volumes:
      - ./src:/src/